{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "## Process Sequences\n",
    "\n",
    "Recurrent neural network is a type of network architecture that accepts variable input and variable outputs.\n",
    "\n",
    "![sequences](sequence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "We can process a sequence of vectors **x** y applying a **recurrence formula** at every time step:\n",
    "\n",
    "$$\n",
    "h_{t} = f_{W}(h_{t - 1}, x_{t})\n",
    "$$\n",
    "\n",
    "$h_{t - 1}$ is known as a previous old state and $h_{t}$ is the new state which is also the output.\n",
    "\n",
    "**NOTE**: The same function and the same set of parameters are used at every time step.\n",
    "\n",
    "Let's look at an example of vanilla recurrent neural network:\n",
    "\n",
    "$$\n",
    "h_{t} = f_{W}(h_{t - 1}, x_{t})\n",
    "$$\n",
    "\n",
    "The state is consists of a single hidden vector **h**\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh}h_{t-1} + W_{xh}x_{t})\n",
    "$$\n",
    "\n",
    "The next hidden state is produced through feeding the previous hidden state and input in a non-linearity, tanh in this case.\n",
    "\n",
    "$$\n",
    "y_{t} = W_{hy}h_{t}\n",
    "$$\n",
    "\n",
    "The output is expected to be a transformation of the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "\n",
    "![graph](rnn_graph_1.png)\n",
    "\n",
    "We begin with a zero'ed vector as our hidden state on the left. We feed it into a function along with the first input. When we receive the next input, we take the new state and feed it into the function again along with the second input. This process goes on until the final hidden state which we will use to generate prediction.\n",
    "\n",
    "![graph](rnn_graph_2.png)\n",
    "\n",
    "Note that we use the same set of weights for every time step! The only variables are the input and the hidden state. If you think about the gradient for **W**, the final gradient is the sum of all those time step gradients.\n",
    "\n",
    "\n",
    "![many-to-many](many-to-many.png)\n",
    "\n",
    "We compute the loss for every time step and at the end we simply sum up the loss of all the time steps and count that as our total loss of the network.\n",
    "\n",
    "![many-to-one](many-to-one.png)\n",
    "\n",
    "![one-to-many](one-to-many.png)\n",
    "\n",
    "### Sequence to Sequence\n",
    "This is equivalent to combining two architecture together, first begin with **many-to-one** as an encoding layer and then **one-to-many** as a decoding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
