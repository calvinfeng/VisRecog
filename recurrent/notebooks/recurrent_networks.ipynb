{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "## Process Sequences\n",
    "\n",
    "Recurrent neural network is a type of network architecture that accepts variable input and variable outputs.\n",
    "\n",
    "![sequences](sequence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "We can process a sequence of vectors **x** applying a **recurrence formula** at every time step:\n",
    "\n",
    "$$\n",
    "h_{t} = f_{W}(h_{t - 1}, x_{t})\n",
    "$$\n",
    "\n",
    "$h_{t - 1}$ is known as a previous old state and $h_{t}$ is the new state which is also the output.\n",
    "\n",
    "**NOTE**: The same function and the same set of parameters are used at every time step.\n",
    "\n",
    "Let's look at an example of vanilla recurrent neural network:\n",
    "\n",
    "$$\n",
    "h_{t} = f_{W}(h_{t - 1}, x_{t})\n",
    "$$\n",
    "\n",
    "The state is consists of a single hidden vector **h**\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh}h_{t-1} + W_{xh}x_{t})\n",
    "$$\n",
    "\n",
    "The next hidden state is produced through feeding the previous hidden state and input in a non-linearity, tanh in this case.\n",
    "\n",
    "$$\n",
    "y_{t} = W_{hy}h_{t}\n",
    "$$\n",
    "\n",
    "The output is expected to be a transformation of the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Computational Graph\n",
    "\n",
    "![graph](rnn_graph_1.png)\n",
    "\n",
    "We begin with a zero'ed vector as our hidden state on the left. We feed it into a function along with the first input. When we receive the next input, we take the new state and feed it into the function again along with the second input. This process goes on until the final hidden state which we will use to generate prediction.\n",
    "\n",
    "![graph](rnn_graph_2.png)\n",
    "\n",
    "Note that we use the same set of weights for every time step! The only variables are the input and the hidden state. If you think about the gradient for **W**, the final gradient is the sum of all those time step gradients.\n",
    "\n",
    "\n",
    "![many-to-many](many-to-many.png)\n",
    "\n",
    "We compute the loss for every time step and at the end we simply sum up the loss of all the time steps and count that as our total loss of the network.\n",
    "\n",
    "![many-to-one](many-to-one.png)\n",
    "\n",
    "![one-to-many](one-to-many.png)\n",
    "\n",
    "### Sequence to Sequence\n",
    "This is equivalent to combining two architecture together, first begin with **many-to-one** as an encoding layer and then **one-to-many** as a decoding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example\n",
    "### Training Time\n",
    "Suppose that we have a character-level language model. The list of possible vocabularies is `['h', 'e', 'l', 'o']`.\n",
    "An example training sequence is `hello`.\n",
    "\n",
    "![lang-model](lang-model.png)\n",
    "\n",
    "The same output from hidden layer is being fed to output layer and the next hidden layer, as noted above that $y_{t}$ is a product of $W_{hy}$ and $h_{t}$. Since we know what we are expecting, we can backpropagate the cost and update weights.\n",
    "\n",
    "### Test Time\n",
    "At test time, we sample characters one at a time and feed it back to the model to produce a whole sequence of characters. We seed the word with a prefix like the letter **h** in this case. The output is a softmax vector which represents probability. We can use it as a probaility distribution and perform sampling. That means EACH character has some chance to be selected.\n",
    "\n",
    "![lang-model](lang-model-test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Backpropagation\n",
    "This idea that we have a sequence and we produce an output at every time stemp of this sequence is called *backpropagation through time*. In the forward pass, we are stepping through step as we produce new hidden states. In the backward pass, we are going *back in time* to compute all of our gradients.\n",
    "\n",
    "### Truncated Backprop\n",
    "This can be very problematic if we want to train a sequence that is very very very long. In practice, what people do that is we **run forward and backward through chunks of sequence** instead of whole sequence.Even though our input sequence may be very very long and even potentially infinite. What we will do is that when we are training the model, we will step forward $S$ amount of steps. Compute a loss only over the $S$ steps and then backpropagate through this subsequence. Now, make a gradient step on the weights.\n",
    "\n",
    "### Diagram\n",
    "![truncated](truncated.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
